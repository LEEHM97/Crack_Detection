{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 모듈 및 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import PIL\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from glob import glob\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ipywidgets import interact\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "# from save import save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>masks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./traincrop\\20160222_081011_1281_721.jpg</td>\n",
       "      <td>./traincrop\\20160222_081011_1281_721.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./traincrop\\20160222_081011_1921_721.jpg</td>\n",
       "      <td>./traincrop\\20160222_081011_1921_721.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./traincrop\\20160222_081011_1_361.jpg</td>\n",
       "      <td>./traincrop\\20160222_081011_1_361.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./traincrop\\20160222_081011_1_721.jpg</td>\n",
       "      <td>./traincrop\\20160222_081011_1_721.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./traincrop\\20160222_081011_641_361.jpg</td>\n",
       "      <td>./traincrop\\20160222_081011_641_361.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     images  \\\n",
       "0  ./traincrop\\20160222_081011_1281_721.jpg   \n",
       "1  ./traincrop\\20160222_081011_1921_721.jpg   \n",
       "2     ./traincrop\\20160222_081011_1_361.jpg   \n",
       "3     ./traincrop\\20160222_081011_1_721.jpg   \n",
       "4   ./traincrop\\20160222_081011_641_361.jpg   \n",
       "\n",
       "                                      masks  \n",
       "0  ./traincrop\\20160222_081011_1281_721.png  \n",
       "1  ./traincrop\\20160222_081011_1921_721.png  \n",
       "2     ./traincrop\\20160222_081011_1_361.png  \n",
       "3     ./traincrop\\20160222_081011_1_721.png  \n",
       "4   ./traincrop\\20160222_081011_641_361.png  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"./traincrop/\"\n",
    "images_paths = glob(data_dir + \"*.jpg\")\n",
    "masks_paths = glob(data_dir + \"*.png\")\n",
    "\n",
    "images_paths = sorted([str(p) for p in images_paths])\n",
    "masks_paths = sorted([str(p) for p in masks_paths])\n",
    "\n",
    "df = pd.DataFrame({'images': images_paths, 'masks': masks_paths})\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1656, Validation size: 184\n"
     ]
    }
   ],
   "source": [
    "train, valid = train_test_split(df, test_size=0.1, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"Train size: {len(train)}, Validation size: {len(valid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf68b630fbf4b4bb431129659cebd71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='index', max=1839), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(index=(0, len(df)-1))\n",
    "def show_images(index=0):\n",
    "    image = cv2.imread(df.iloc[index].images)\n",
    "    mask = cv2.imread(df.iloc[index].masks)\n",
    "\n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.subplot(121)\n",
    "    plt.title(\"image\")\n",
    "    plt.imshow(image)\n",
    "    plt.subplot(122)\n",
    "    plt.title(\"mask\")\n",
    "    plt.imshow(mask)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터셋 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import albumentations as A\n",
    "\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = [\n",
    "    A.RandomRain(brightness_coefficient=0.9, drop_width=1, blur_value=2, p=1),\n",
    "    # A.RandomShadow(num_shadows_lower=1, num_shadows_upper=1, shadow_dimension=5, shadow_roi=(0, 0.5, 1, 1), p=1),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, brightness_by_max=True, p=1),\n",
    "    A.CLAHE(clip_limit=2, tile_grid_size=(8, 8), p=1),\n",
    "    # A.RGBShift(r_shift_limit=15, g_shift_limit=15,b_shift_limit=15, p=1),\n",
    "    ToTensorV2()\n",
    "    ]\n",
    "\n",
    "train_transform = A.Compose(train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.loc[index].squeeze()\n",
    "        image_path = row['images']\n",
    "        mask_path = row['masks']\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        mask = cv2.imread(mask_path, 0) // 255\n",
    "        \n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = SegmentationDataset(df, train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 360, 640])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84da31bb9b3543b2af4390a0cce54eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='index', max=1839), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(index=(0, len(df)-1))\n",
    "def show_images(index=0):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.axis=(\"off\")\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(train_ds[index][0].cpu().detach().numpy().transpose(1,2,0))\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(np.squeeze(train_ds[index][1]).cpu().detach().numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('usePytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d67afed350c528055f2528b65dcee38560ce04848f0ceec4c78743b97b05f9fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
